<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-08-27T02:20:56+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ajith Raghavan</title><subtitle>Ajith Raghavan&apos;s personal Tech blog sharing insights and tutorials about Technology, Programming, AI, Machine Learning</subtitle><author><name>Ajith Raghavan</name></author><entry><title type="html">🏞️ CNN Basics</title><link href="http://localhost:4000/blog/cnn-intro/" rel="alternate" type="text/html" title="🏞️ CNN Basics" /><published>2025-08-26T00:00:00+05:30</published><updated>2025-08-26T00:00:00+05:30</updated><id>http://localhost:4000/blog/cnn-intro</id><content type="html" xml:base="http://localhost:4000/blog/cnn-intro/"><![CDATA[<h2 id="-convnet">⿻ ConvNet</h2>

<p>☝️- This should be best(available) Emoji 😉 - <a href="https://ajithraghavan.github.io/blog/emoji-generator/">Emoji Generator</a></p>

<p>Convolution Neural Network (CNN or ConvNet) is a type of Neural Network, which is good at processing image</p>

<p>We do Convolution(Cross Correlation), by following BUT much larger and bigger</p>

<p>Yes, now also, still competitive to ViT(Vision Transformer)</p>

<p>With ConvNet we can do,</p>

<h3 id="image-classification">Image Classification</h3>

<p>Where we classify image, like what contains in an image</p>

<p>Some Models are:</p>

<ol>
  <li>AlexNet</li>
  <li>VGGNet(16, 19)</li>
  <li>Inception(Very different approach)</li>
  <li>ResNet(Responsible for Residual Connection which is used in Transformers too!)</li>
</ol>

<h3 id="object-detection">Object Detection</h3>

<p>Detect and draw Bounding Box around the objects in the image like Car, Pedastrian</p>

<p>Some Models are:</p>

<ol>
  <li>R-CNN</li>
  <li>YOLO
    <ul>
      <li>YOLO is such an amazing Paper covers cool topics like:
        <ol>
          <li>Intersection Over Union(IoU)</li>
          <li>Non-Max Suppression</li>
          <li>Anchor Boxes</li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<h3 id="image-segmentation">Image Segmentation</h3>

<p>Where we paint the color around possible Objects in an image</p>

<p>Some Models are:</p>

<ol>
  <li>FCN</li>
  <li>U‑Net 😍</li>
</ol>

<p>And much more CNN Models</p>

<p>Let us discuss more in another Post!</p>

<h3 id="what-deep-convnet-learns">What Deep ConvNet Learns?</h3>

<p>But, crucial question CNN have these layers, what these layers learn?</p>

<p>Let us Consider this image and let us consider that this CNN has 4 Layers</p>

<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/01/image-classification-model-6731d4bb5c389.webp" alt="Image" />
Source : <a href="analyticsvidhya.com">Analytics Vidhya</a></p>

<p>It might Learn the Features like,</p>

<p>First Layer :  “Straight Line”, “Cross Lines”, etc</p>

<p>Second Layer : “Curves”, “ Common Pattern”, etc</p>

<p>Third Layer : “Cat Nose”, “Dog Ears” etc</p>

<p>Fourth Layer : “Cat Face”, “Dog Face” etc</p>

<p>Earlier Layers learns small patters like Lines, Shapes and</p>

<p>Deep Layers will Learn Complex Features like Nose, Ears, even Face etc</p>]]></content><author><name>Ajith Raghavan</name></author><category term="🧑‍💻 ML" /><category term="Neural Network" /><category term="CNN" /><summary type="html"><![CDATA[CNN Basics]]></summary></entry><entry><title type="html">Try My ~3.5 Million Parameters Model Instead for ✨😉</title><link href="http://localhost:4000/blog/emoji-generator-copy/" rel="alternate" type="text/html" title="Try My ~3.5 Million Parameters Model Instead for ✨😉" /><published>2025-08-24T00:00:00+05:30</published><updated>2025-08-24T00:00:00+05:30</updated><id>http://localhost:4000/blog/emoji-generator%20copy</id><content type="html" xml:base="http://localhost:4000/blog/emoji-generator-copy/"><![CDATA[<h2 id="intro">😊 Introduction</h2>

<p>I thought to Build a Machine Learning Model to generate Emojis for My Writing content</p>

<p>Like when typing some sentences, I think I need relevant Emojis in the sentences, as it will give more expressiveness, right</p>

<p>And also because currently I am searching in Web(using some website like Emoji Finder etc., it is based on keywords only) to pick Emojis</p>

<p>So I decided to Develop a <strong>LSTM Model</strong> to automatically generate Emoji for My Sentences</p>

<h2 id="demo">🎥 First Demo</h2>
<p><img src="/assets/images/2025-08-24-emoji-generator/output.gif" alt="Demo Video" /></p>

<h2 id="-explanation">🧑‍🏫 Explanation</h2>

<p>When the user types the paragraph, for each paragraph the Model predicts the approximate Emoji</p>

<p>For example, if you have 5 sentences in a paragraph, the Model will generate 5 Emojis</p>

<h2 id="technical-details">⚙️ Technical Details</h2>

<p>I currently have totally 20 Emojis ❤, 😍, 😂, 💕, 🔥, 😊, 😎, ✨, 💙, 😘, 📷, 🇺🇸, ☀, 💜, 😉, 💯, 😁, 🎄, 📸, 😜</p>

<p>And used Dataset from <a href="https://huggingface.co/datasets/cardiffnlp/tweet_eval">Hugging Face</a> and used Google Colab for Training</p>

<h3 id="model-selection">🎯 Model Selection</h3>

<p>I have decided to use BiLSTM(Bidirectional LSTM(Long Short-Term Memory))</p>

<p>We could have just used <strong>LSTM</strong> rather then <strong>BiLSTM</strong> as we are not doing Seq2Seq Task, like we are not translating English to Emojis</p>

<p>But, here <strong>BiLSTM</strong> improves the performance by at least 10%</p>

<p>So, decided to use <strong>BiLSTM</strong></p>

<blockquote>
  <p>Note :</p>
</blockquote>

<blockquote>
  <p>Here we could have used BiLSTM with Attention Mechanism or even  Fine-Tuned Pre Trained Model like BERT</p>
</blockquote>

<blockquote>
  <p>But decided to use BiLSTM</p>
</blockquote>

<h3 id="architecture">🏗️ Architecture</h3>

<ol>
  <li>First have used Embedding Layer</li>
  <li>Then, BiLSTM with 2 Layers</li>
  <li>Dropout for Regularization</li>
  <li>Then Linear Layer with 20 output units for each Emojis
    <ul>
      <li>Note :</li>
      <li>We are using BiLSTM, so it will hidden states from both the directions</li>
      <li>So, before passing to Linear Layer we must concatenate both the hidden states</li>
    </ul>
  </li>
</ol>

<h3 id="hyperparameters">🎲 Hyperparameters</h3>

<ol>
  <li>I have used the Maximum length of the Sequence to be <code class="language-plaintext highlighter-rouge">50</code></li>
  <li>Embedding Dimension as <code class="language-plaintext highlighter-rouge">128</code></li>
  <li>Hidden Dimension for LSTM States to be <code class="language-plaintext highlighter-rouge">128</code></li>
  <li>As stated before, we have chosen Number of Layers to be 2</li>
  <li>Learning Rate α : 0.0005</li>
</ol>

<h3 id="accuracy-and-important-study">🚨 Accuracy and Important Study</h3>

<p>Regarding Accuracy, it is relatively low although with <strong>BiLSTM</strong> and even low with <strong>Unidirectional LSTM</strong></p>

<p>Was tweaking and trying to improve the performance then LATER ONLY realized that data has Class Imbalance Problem</p>

<p>Where ~21% of the Data has “❤” which dominates most and followed by ~10% of Data has “😍” and so on… then “😜” was barely minimum</p>

<h4 id="️-important-action">⚠️ Important action</h4>

<p>🥇 First Rule : Data in Training Set, Development Set and Test Set should follow the same Distribution</p>

<p>Followed by, may be here, we can Augmented Data Synthetically, or may be with some other Architecture so on</p>

<p>Yes, we can also increase Accuracy with the same Data but as of now saved that for later</p>

<h3 id="little-history">📜 Little History</h3>

<p>I have used Regular LSTM and saw the Model was Overfitting</p>

<p>So, of various Hyperparameters(still overfitting) after few tweaking various Hyperparameters along with  BiLSTM showed little improvement and continued and seems to work fine</p>

<p>Thanks for reading</p>]]></content><author><name>Ajith Raghavan</name></author><category term="🧑‍💻 ML" /><category term="Neural Network" /><category term="RNN" /><category term="LSTM" /><summary type="html"><![CDATA[Try My ~3.5 Million Parameters Model Instead for ✨😉 for Emoji Generation]]></summary></entry><entry><title type="html">First Post</title><link href="http://localhost:4000/blog/first-post/" rel="alternate" type="text/html" title="First Post" /><published>2025-08-24T00:00:00+05:30</published><updated>2025-08-24T00:00:00+05:30</updated><id>http://localhost:4000/blog/first-post</id><content type="html" xml:base="http://localhost:4000/blog/first-post/"><![CDATA[<h2 id="intro">About Me</h2>

<p>👋 Hi, I am Ajith Raghavan Welcome to my tech blog where I share insights and tutorials on technology, programming, AI, and machine learning.</p>

<p>Let us explore 🚀</p>

<h2 id="contact">About This Blog</h2>

<p>This blog was created in 2025 as a platform to share knowledge, insights, and tutorials about technologies I work with and find interesting.</p>

<p>Here you’ll find articles about:</p>

<ul>
  <li>Programming languages and frameworks</li>
  <li>AI and Machine Learning concepts</li>
  <li>Technical tutorials and guides</li>
  <li>Thoughts on emerging Technologies</li>
</ul>]]></content><author><name>Ajith Raghavan</name></author><category term="🧑‍💻 Tech" /><category term="🙋 Intro" /><summary type="html"><![CDATA[Welcome to my tech blog where I share insights and tutorials on technology, programming, AI, and machine learning.]]></summary></entry></feed>